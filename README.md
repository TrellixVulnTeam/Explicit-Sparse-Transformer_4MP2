# Explicit-Sparse-Transformer
In  Explicit Sparse Transformer, we propose an algorithm which sparse attention weights in transformer according to their activations.

2020 1/4  we upload code for explicit sparse transformer in tensor2tensor and fairseq, see t2t_envi_est.sh and fairseq_deen_est.sh for details.
2021 1/14  we address an import error related to SparseActivatedMultiheadAttention
